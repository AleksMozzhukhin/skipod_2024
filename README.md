# Суперкомпьютеры и параллельная обработка данных

Проект по разработке и сравнению параллельных алгоритмов для решения трёхмерного уравнения Лапласа методом Якоби.

## Описание проекта

Данный проект представляет собой практическое задание №37 по курсу "Суперкомпьютеры и параллельная обработка данных" МГУ имени М.В. Ломоносова. Целью работы является модификация исходной последовательной программы для решения задачи численного моделирования с использованием параллельных технологий OpenMP и MPI.

### Задача

Программа решает трёхмерное уравнение Лапласа на кубической области размером N×N×N, используя итерационный метод Якоби для нахождения стационарного распределения значений внутри области с заданными краевыми условиями.

## Структура проекта

```
.
├── src/
│   ├── var37.c          # Оригинальная последовательная версия
│   ├── var37_for.c      # OpenMP версия с директивой for
│   ├── var37_task.c     # OpenMP версия с директивой task
│   └── var37_mpi.c      # MPI версия
├── shell_scripts/             # Shell скрипты для тестирования
└── README.md
```

## Реализованные версии

### 1. Оригинальная версия (`var37.c`)
- Последовательное выполнение алгоритма
- Базовая реализация метода Якоби
- Эталон для сравнения производительности

### 2. OpenMP версия с директивой for (`var37_for.c`)
**Основные особенности:**
- `#pragma omp parallel for collapse(3)` для параллелизации трёх вложенных циклов
- Использование `schedule(static)` для равномерного распределения итераций
- `reduction(max:local_eps)` для параллельного вычисления максимальной ошибки
- `#pragma omp critical` для синхронизации обновления глобальной переменной eps

### 3. OpenMP версия с директивой task (`var37_task.c`)
**Основные особенности:**
- `#pragma omp task` для создания независимых задач
- Динамическое распределение нагрузки между потоками
- `#pragma omp taskwait` для синхронизации выполнения задач
- Ручное разбиение циклов на задачи с учётом количества потоков

### 4. MPI версия (`var37_mpi.c`)
**Основные особенности:**
- Распределение данных между процессами по размерности k
- Обмен граничными слоями (`halo_up` и `halo_down`) через `MPI_Sendrecv()`
- `MPI_Allreduce()` для вычисления глобальной ошибки сходимости
- `MPI_Reduce()` для объединения результатов верификации

## Параметры тестирования

### Размеры данных
- **N = 66** (малые данные)
- **N = 130** (средние данные)
- **N = 258** (большие данные)

### Параметры сходимости
- Максимально допустимая ошибка: `maxeps = 0.1e-7`
- Максимальное число итераций: `itmax = 100`

### Количество потоков/процессов
Тестирование проводилось с 1, 2, 3, 4, 5, 6, 7, 8, 10, 20, 40, 60, 80, 100, 120, 140, 160 потоками/процессами.

## Сборка и запуск

### Требования
- GCC с поддержкой OpenMP
- MPI реализация (OpenMPI или MPICH)
- POSIX-совместимая система

### Компиляция

```bash
# Оригинальная версия
gcc -o var37 var37.c -lm

# OpenMP версия (for)
gcc -fopenmp -o var37_for var37_for.c -lm

# OpenMP версия (task)
gcc -fopenmp -o var37_task var37_task.c -lm

# MPI версия
mpicc -o var37_mpi var37_mpi.c -lm
```

### Запуск

```bash
# Оригинальная версия
./var37

# OpenMP версии
export OMP_NUM_THREADS=4
./var37_for
./var37_task

# MPI версия
mpirun -np 4 ./var37_mpi
```

## Результаты производительности

### Основные выводы

#### Когда все программы работают хорошо:
- При малом количестве потоков/процессов (1-4)
- На средних данных (N = 130) с 10-20 потоками/процессами

#### Когда все программы работают плохо:
- Малые данные (N = 66) с большим числом потоков/процессов (> 40)
- Очень большое количество потоков/процессов (> 100) на малых данных

#### Оптимальные области применения:

**OpenMP (for):**
- Лучше для малых и средних данных (N = 66, 130)
- Оптимально при количестве потоков < 20
- Стабильное распределение итераций, минимальные накладные расходы

**OpenMP (task):**
- Превосходит на больших данных (N = 258)
- Эффективно при среднем количестве потоков (20-40)
- Гибкое динамическое распределение нагрузки

**MPI:**
- Значительно превосходит на больших данных (N = 258) при > 40 процессах
- Время выполнения сокращается более чем в 500 раз для больших данных
- Лучшая масштабируемость среди всех версий

### Производительность (примеры для N = 258)

| Версия | 1 поток/процесс | 40 потоков/процессов | Ускорение |
|--------|-----------------|---------------------|-----------|
| Оригинальная | 87.66 с | - | 1.0x |
| OpenMP (for) | 211.93 с | 20.20 с | 10.5x |
| OpenMP (task) | 407.92 с | 21.58 с | 18.9x |
| MPI | 942.31 с | 16.46 с | 57.3x |

## Анализ масштабируемости

Программы демонстрируют различные характеристики масштабируемости в зависимости от размера данных и количества вычислительных ресурсов. Подробные графики и анализ производительности представлены в отчёте.

### Ограничения масштабируемости:
1. **Накладные расходы на коммуникацию** (особенно в MPI)
2. **Создание и управление задачами** (в OpenMP task)
3. **Неравномерное распределение нагрузки** при большом числе потоков/процессов
4. **Ограничения аппаратуры** (количество физических ядер, пропускная способность памяти)

## Рекомендации по выбору версии

- **Для малых и средних задач с быстрым решением:** OpenMP (for)
- **Для сложных задач с непредсказуемой нагрузкой:** OpenMP (task)
- **Для больших данных, требующих высокой масштабируемости:** MPI

т создан в учебных целях для курса "Суперкомпьютеры и параллельная обработка данных".
